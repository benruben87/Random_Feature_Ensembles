{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3078ce00-eb46-4889-ba72-5e7c48e51fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# Import your custom libraries\n",
    "import EnsRFTheory  # Ensure EnsRFTheory.py is in the same directory or Python path\n",
    "import EnsembleRFs\n",
    "import auxFuncs\n",
    "import DatasetMaker\n",
    "import LearningCurveExperiments\n",
    "\n",
    "# Set device to GPU if available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "831fbc09-fbb2-4d80-86cb-b68e7b4acdcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'LearningCurveExperiments' from '/n/home07/bruben/Simulations/Ensemble_DeepLearning/RandomFeatures/LearningCurveExperiments.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(EnsRFTheory)\n",
    "importlib.reload(EnsembleRFs)\n",
    "importlib.reload(auxFuncs)\n",
    "importlib.reload(DatasetMaker)\n",
    "importlib.reload(LearningCurveExperiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e493923-6a51-47ab-93b8-16642c9b7660",
   "metadata": {},
   "source": [
    "# Figure 1: Sample and Size Monotonicity in Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0852580-f4b9-4206-b28f-86f6eab1c5b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## CIFAR Experiments Sweep P with N, K fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "bae1acd9-9b82-44a6-bfe8-b626d0acb1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_trials = 50\n",
    "KVals = [1, 2, 4, 8]\n",
    "N = 256\n",
    "lamVals = np.logspace(-3, 1, 50)\n",
    "Plower = 1\n",
    "Pupper = 4\n",
    "P_list = [int(i) for i in np.unique(np.round(np.logspace(Plower, Pupper, 20)))]\n",
    "P_list_theory =np.logspace(Plower, Pupper, 200)\n",
    "PTest = 10000\n",
    "nonlinearity = torch.relu  # Relu random features.\n",
    "ensErrFuncs = [(auxFuncs.mean, auxFuncs.SquareError), (auxFuncs.mean, auxFuncs.SgnErrorRate), (auxFuncs.majorityVote, auxFuncs.SgnErrorRate), (auxFuncs.median, auxFuncs.SgnErrorRate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4111fea0-4c2c-442b-8176-cbabb6eb64af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/n/holystore01/LABS/pehlevan_lab/Everyone/cifar' #Location of the cifar 10 dataset.\n",
    "class_groups =  [[0,1,7,8,9], [2,3,4,5,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e91630db-aff5-48a8-8a72-30953d9048ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "X_train_np, y_train_np, X_test_np, y_test_np = DatasetMaker.get_binarized_CIFAR10(data_root, class_groups, flatten=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "10288aec-e8e5-4e45-8335-f7146c019d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X_train_np.shape[1]\n",
    "Fvar = 2/D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "fdc22ecd-06a2-4997-b96e-33120d9c54b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to torch tensors\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float64).cuda()\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.float64).cuda().reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test_np, dtype=torch.float64).cuda()\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.float64).cuda().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "36f2c6b2-cbf6-4131-bdf9-6f3eab8d64ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 1/50\n",
      "Completed trial 1/50\n",
      "Starting trial 2/50\n",
      "Completed trial 2/50\n",
      "Starting trial 3/50\n",
      "Completed trial 3/50\n",
      "Starting trial 4/50\n",
      "Completed trial 4/50\n",
      "Starting trial 5/50\n",
      "Completed trial 5/50\n",
      "Starting trial 6/50\n",
      "Completed trial 6/50\n",
      "Starting trial 7/50\n",
      "Completed trial 7/50\n",
      "Starting trial 8/50\n",
      "Completed trial 8/50\n",
      "Starting trial 9/50\n",
      "Completed trial 9/50\n",
      "Starting trial 10/50\n",
      "Completed trial 10/50\n",
      "Starting trial 11/50\n",
      "Completed trial 11/50\n",
      "Starting trial 12/50\n",
      "Completed trial 12/50\n",
      "Starting trial 13/50\n",
      "Completed trial 13/50\n",
      "Starting trial 14/50\n",
      "Completed trial 14/50\n",
      "Starting trial 15/50\n",
      "Completed trial 15/50\n",
      "Starting trial 16/50\n",
      "Completed trial 16/50\n",
      "Starting trial 17/50\n",
      "Completed trial 17/50\n",
      "Starting trial 18/50\n",
      "Completed trial 18/50\n",
      "Starting trial 19/50\n",
      "Completed trial 19/50\n",
      "Starting trial 20/50\n",
      "Completed trial 20/50\n",
      "Starting trial 21/50\n",
      "Completed trial 21/50\n",
      "Starting trial 22/50\n",
      "Completed trial 22/50\n",
      "Starting trial 23/50\n",
      "Completed trial 23/50\n",
      "Starting trial 24/50\n",
      "Completed trial 24/50\n",
      "Starting trial 25/50\n",
      "Completed trial 25/50\n",
      "Starting trial 26/50\n",
      "Completed trial 26/50\n",
      "Starting trial 27/50\n",
      "Completed trial 27/50\n",
      "Starting trial 28/50\n",
      "Completed trial 28/50\n",
      "Starting trial 29/50\n",
      "Completed trial 29/50\n",
      "Starting trial 30/50\n",
      "Completed trial 30/50\n",
      "Starting trial 31/50\n",
      "Completed trial 31/50\n",
      "Starting trial 32/50\n",
      "Completed trial 32/50\n",
      "Starting trial 33/50\n",
      "Completed trial 33/50\n",
      "Starting trial 34/50\n",
      "Completed trial 34/50\n",
      "Starting trial 35/50\n",
      "Completed trial 35/50\n",
      "Starting trial 36/50\n",
      "Completed trial 36/50\n",
      "Starting trial 37/50\n",
      "Completed trial 37/50\n",
      "Starting trial 38/50\n",
      "Completed trial 38/50\n",
      "Starting trial 39/50\n",
      "Completed trial 39/50\n",
      "Starting trial 40/50\n",
      "Completed trial 40/50\n",
      "Starting trial 41/50\n",
      "Completed trial 41/50\n",
      "Starting trial 42/50\n",
      "Completed trial 42/50\n",
      "Starting trial 43/50\n",
      "Completed trial 43/50\n",
      "Starting trial 44/50\n",
      "Completed trial 44/50\n",
      "Starting trial 45/50\n",
      "Completed trial 45/50\n",
      "Starting trial 46/50\n",
      "Completed trial 46/50\n",
      "Starting trial 47/50\n",
      "Completed trial 47/50\n",
      "Starting trial 48/50\n",
      "Completed trial 48/50\n",
      "Starting trial 49/50\n",
      "Completed trial 49/50\n",
      "Starting trial 50/50\n",
      "Completed trial 50/50\n"
     ]
    }
   ],
   "source": [
    "test_errors = LearningCurveExperiments.train_random_feature_models_fixN(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    num_trials, \n",
    "    KVals, \n",
    "    N, \n",
    "    lamVals, \n",
    "    P_list, \n",
    "    ensErrFuncs, \n",
    "    nonlinearity=nonlinearity, \n",
    "    Fvar = Fvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "93cfa545-a58a-44e2-a586-e6ba39509bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importlib.reload(LearningCurveExperiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "13b17fb9-2a45-477b-a4b1-8e3b3dc37820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical learning curves computed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the subset size and file directory\n",
    "subset_size = 30000  # Set your specific subset size\n",
    "save_dir = \"KernelSpectra\"\n",
    "\n",
    "# Load the saved dictionary for the given subset size\n",
    "with open(os.path.join(save_dir, f'CIFAR_kernel_results_{subset_size}.pkl'), 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# Retrieve eigenvalues, wbar, and sigma_eps from the dictionary\n",
    "eigvals = results['eigvals'].reshape(-1)\n",
    "wbar = results['wbar'].reshape(-1)\n",
    "sigma_eps = results['sigma_eps']\n",
    "\n",
    "# Compute theoretical learning curves using the retrieved eigenvalues, wbar, and sigma_eps\n",
    "test_errors_theory = LearningCurveExperiments.compute_theoretical_learning_curves_fixN(\n",
    "    Sigma=eigvals,       # Sigma corresponds to the kernel eigenvalues\n",
    "    wbar=wbar,           # Ground truth weights from CIFAR10 task\n",
    "    KVals=KVals,         # Number of ensemble models\n",
    "    lamVals=lamVals,     # Ridge regularization values\n",
    "    P_list=P_list_theory,       # Different training sample sizes\n",
    "    N=N,                 # Total number of random features\n",
    "    sigma_eps = sigma_eps        #noise level in the task.\n",
    ")\n",
    "\n",
    "# Print or save the theoretical learning curves as needed\n",
    "print(\"Theoretical learning curves computed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "51586de8-1476-42e6-97ea-488e8b3318ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment saved to /n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features/RF_CIFAR_N256_VSP.pkl\n"
     ]
    }
   ],
   "source": [
    "# Define the experiment output and parameters as a dictionary\n",
    "experiment_results = {\n",
    "    'num_trials': num_trials,\n",
    "    'KVals': KVals,\n",
    "    'N': N,\n",
    "    'lamVals': lamVals,\n",
    "    'P_list': P_list,\n",
    "    'P_list_theory': P_list_theory,\n",
    "    'PTest': PTest,\n",
    "    'nonlinearity': nonlinearity,\n",
    "    'ensErrFuncs': ensErrFuncs,\n",
    "    'class_groups': class_groups,  # Class groups for CIFAR-10 binarization\n",
    "    'test_errors': test_errors,  # Numerical results from random feature models\n",
    "    'test_errors_theory': test_errors_theory,  # Theoretical results\n",
    "    'ensErrFuncs': ensErrFuncs\n",
    "}\n",
    "\n",
    "# Create a descriptive filename with a timestamp to avoid overwriting\n",
    "#timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#filename = f'RF_CIFAR_{timestamp}.pkl'\n",
    "filename = f'RF_CIFAR_N{N}_VSP.pkl'\n",
    "\n",
    "# Define the save path\n",
    "save_path = '/n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open(os.path.join(save_path, filename), 'wb') as f:\n",
    "    pickle.dump(experiment_results, f)\n",
    "\n",
    "print(f\"Experiment saved to {os.path.join(save_path, filename)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b6076-fb58-4a7a-9f0a-dea0aec6b92c",
   "metadata": {},
   "source": [
    "## CIFAR Experiments Sweeping N and K with P fixed, many ensErrFuncs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1c301451-c483-465d-95b8-f138017021db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_trials = 50\n",
    "Nlower=1\n",
    "Nupper=4\n",
    "NVals = [int(i) for i in np.unique(np.round(np.logspace(Nlower, Nupper, 20)))]  # List of M values to sweep over\n",
    "NVals_theory =np.logspace(Nlower, Nupper, 200)\n",
    "KVals = [1, 2, 4, 8]  # List of K values\n",
    "lamVals = np.logspace(-3, 1, 10)\n",
    "P = 256  # Fixed training sample size\n",
    "PTest = 10000\n",
    "nonlinearity = torch.relu  # ReLU random features.\n",
    "ensErrFuncs = [(auxFuncs.mean, auxFuncs.SquareError), (auxFuncs.mean, auxFuncs.SgnErrorRate), (auxFuncs.majorityVote, auxFuncs.SgnErrorRate), (auxFuncs.median, auxFuncs.SgnErrorRate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d8db69a6-1542-4c18-b693-113e6dcfffc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Starting trial 1/50\n",
      "Completed trial 1/50\n",
      "Starting trial 2/50\n",
      "Completed trial 2/50\n",
      "Starting trial 3/50\n",
      "Completed trial 3/50\n",
      "Starting trial 4/50\n",
      "Completed trial 4/50\n",
      "Starting trial 5/50\n",
      "Completed trial 5/50\n",
      "Starting trial 6/50\n",
      "Completed trial 6/50\n",
      "Starting trial 7/50\n",
      "Completed trial 7/50\n",
      "Starting trial 8/50\n",
      "Completed trial 8/50\n",
      "Starting trial 9/50\n",
      "Completed trial 9/50\n",
      "Starting trial 10/50\n",
      "Completed trial 10/50\n",
      "Starting trial 11/50\n",
      "Completed trial 11/50\n",
      "Starting trial 12/50\n",
      "Completed trial 12/50\n",
      "Starting trial 13/50\n",
      "Completed trial 13/50\n",
      "Starting trial 14/50\n",
      "Completed trial 14/50\n",
      "Starting trial 15/50\n",
      "Completed trial 15/50\n",
      "Starting trial 16/50\n",
      "Completed trial 16/50\n",
      "Starting trial 17/50\n",
      "Completed trial 17/50\n",
      "Starting trial 18/50\n",
      "Completed trial 18/50\n",
      "Starting trial 19/50\n",
      "Completed trial 19/50\n",
      "Starting trial 20/50\n",
      "Completed trial 20/50\n",
      "Starting trial 21/50\n",
      "Completed trial 21/50\n",
      "Starting trial 22/50\n",
      "Completed trial 22/50\n",
      "Starting trial 23/50\n",
      "Completed trial 23/50\n",
      "Starting trial 24/50\n",
      "Completed trial 24/50\n",
      "Starting trial 25/50\n",
      "Completed trial 25/50\n",
      "Starting trial 26/50\n",
      "Completed trial 26/50\n",
      "Starting trial 27/50\n",
      "Completed trial 27/50\n",
      "Starting trial 28/50\n",
      "Completed trial 28/50\n",
      "Starting trial 29/50\n",
      "Completed trial 29/50\n",
      "Starting trial 30/50\n",
      "Completed trial 30/50\n",
      "Starting trial 31/50\n",
      "Completed trial 31/50\n",
      "Starting trial 32/50\n",
      "Completed trial 32/50\n",
      "Starting trial 33/50\n",
      "Completed trial 33/50\n",
      "Starting trial 34/50\n",
      "Completed trial 34/50\n",
      "Starting trial 35/50\n",
      "Completed trial 35/50\n",
      "Starting trial 36/50\n",
      "Completed trial 36/50\n",
      "Starting trial 37/50\n",
      "Completed trial 37/50\n",
      "Starting trial 38/50\n",
      "Completed trial 38/50\n",
      "Starting trial 39/50\n",
      "Completed trial 39/50\n",
      "Starting trial 40/50\n",
      "Completed trial 40/50\n",
      "Starting trial 41/50\n",
      "Completed trial 41/50\n",
      "Starting trial 42/50\n",
      "Completed trial 42/50\n",
      "Starting trial 43/50\n",
      "Completed trial 43/50\n",
      "Starting trial 44/50\n",
      "Completed trial 44/50\n",
      "Starting trial 45/50\n",
      "Completed trial 45/50\n",
      "Starting trial 46/50\n",
      "Completed trial 46/50\n",
      "Starting trial 47/50\n",
      "Completed trial 47/50\n",
      "Starting trial 48/50\n",
      "Completed trial 48/50\n",
      "Starting trial 49/50\n",
      "Completed trial 49/50\n",
      "Starting trial 50/50\n",
      "Completed trial 50/50\n"
     ]
    }
   ],
   "source": [
    "# Data root and class groups for CIFAR-10 binarization\n",
    "data_root = '/n/holystore01/LABS/pehlevan_lab/Everyone/cifar'  # Location of the CIFAR-10 dataset.\n",
    "class_groups = [[0, 1, 7, 8, 9], [2, 3, 4, 5, 6]]\n",
    "\n",
    "# Load and preprocess CIFAR-10 data\n",
    "X_train_np, y_train_np, X_test_np, y_test_np = DatasetMaker.get_binarized_CIFAR10(\n",
    "    data_root, class_groups, flatten=True, normalize=True\n",
    ")\n",
    "\n",
    "D = X_train_np.shape[1]\n",
    "Fvar = 2 / D\n",
    "\n",
    "# Convert data to torch tensors and move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float64).to(device)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.float64).to(device).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test_np, dtype=torch.float64).to(device)\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.float64).to(device).reshape(-1, 1)\n",
    "\n",
    "# Ensure that the training data is large enough for all trials\n",
    "total_train_samples = P * num_trials\n",
    "if X_train.shape[0] < total_train_samples:\n",
    "    # Optionally, you can augment the training data here\n",
    "    raise ValueError(\"Not enough training samples for the specified number of trials.\")\n",
    "\n",
    "# Run the numerical experiments using the new function\n",
    "test_errors = LearningCurveExperiments.train_random_feature_models_N_K(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    num_trials, NVals, KVals, lamVals, P, ensErrFuncs,\n",
    "    nonlinearity=nonlinearity, Fvar=Fvar\n",
    ")\n",
    "\n",
    "# Compute mean and standard deviation over trials\n",
    "mean_test_errors = np.mean(test_errors, axis=0)\n",
    "std_test_errors = np.std(test_errors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "22aebd45-1dc1-4d7b-84a0-9b18ba02d96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'LearningCurveExperiments' from '/n/home07/bruben/Simulations/Ensemble_DeepLearning/RandomFeatures/LearningCurveExperiments.py'>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(LearningCurveExperiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "22024f8e-793f-44c8-af92-7cf3f02b9faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical learning curves computed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the subset size and file directory\n",
    "subset_size = 30000  # Set your specific subset size\n",
    "save_dir = \"KernelSpectra\"\n",
    "\n",
    "# Load the saved dictionary for the given subset size\n",
    "with open(os.path.join(save_dir, f'CIFAR_kernel_results_{subset_size}.pkl'), 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# Retrieve eigenvalues, wbar, and sigma_eps from the dictionary\n",
    "eigvals = results['eigvals'].reshape(-1)\n",
    "wbar = results['wbar'].reshape(-1)\n",
    "sigma_eps = results['sigma_eps']\n",
    "\n",
    "# Compute theoretical learning curves using the new function\n",
    "test_errors_theory = LearningCurveExperiments.compute_theoretical_learning_curves_N_K(\n",
    "    Sigma=eigvals,       # Sigma corresponds to the kernel eigenvalues\n",
    "    wbar=wbar,           # Ground truth weights from CIFAR-10 task\n",
    "    NVals=NVals_theory,\n",
    "    KVals=KVals,\n",
    "    lamVals=lamVals,\n",
    "    P=P,                 # Fixed training sample size\n",
    "    sigma_eps=0          # Noise level in the task.\n",
    ")\n",
    "\n",
    "print(\"Theoretical learning curves computed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "204510dd-8d8e-4cf0-982c-d419473d4fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment saved to /n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features/RF_CIFAR_P256_VsN.pkl\n"
     ]
    }
   ],
   "source": [
    "# Define the experiment output and parameters as a dictionary\n",
    "experiment_results = {\n",
    "    'num_trials': num_trials,\n",
    "    'NVals': NVals,\n",
    "    'NVals_theory': NVals_theory,\n",
    "    'KVals': KVals,\n",
    "    'lamVals': lamVals,\n",
    "    'P': P,\n",
    "    'PTest': PTest,\n",
    "    'nonlinearity': nonlinearity,\n",
    "    'class_groups': class_groups,  # Class groups for CIFAR-10 binarization\n",
    "    'test_errors': test_errors,          # Numerical results from random feature models\n",
    "    'test_errors_theory': test_errors_theory,  # Theoretical results\n",
    "    'mean_test_errors': mean_test_errors,\n",
    "    'std_test_errors': std_test_errors\n",
    "}\n",
    "\n",
    "# Create a descriptive filename with a timestamp to avoid overwriting\n",
    "filename = f'RF_CIFAR_P{P}_VsN.pkl'\n",
    "\n",
    "# Define the save path\n",
    "save_path = '/n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open(os.path.join(save_path, filename), 'wb') as f:\n",
    "    pickle.dump(experiment_results, f)\n",
    "\n",
    "print(f\"Experiment saved to {os.path.join(save_path, filename)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f1d539-0095-4b58-bc67-9545893dd361",
   "metadata": {},
   "source": [
    "# Figure 2: $E_g$ increase monotonicity with $K$ when $P$ and $M$ are fixed.\n",
    "\n",
    "Modify to determine how Bias and Variance depend on K at optimal ridge.ensErrFuncsreturnBiasVariance="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b66105-be73-42df-8ea3-794a4cc54d9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gaussian Data Experiment Sweeping K with fixed M and P (Multiple P values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bffc2f53-aeb9-4057-a6b1-5be6db8967da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_trials = 10\n",
    "KVals = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "M = 2**10\n",
    "lamVals = np.logspace(-4, 2, 50)\n",
    "P_list = [2**7, 2**10, 2**13]\n",
    "PTest = 10000\n",
    "nonlinearity = None  # Assuming linear model\n",
    "ensErrFuncs = [(auxFuncs.mean, auxFuncs.SquareError)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5342a81c-dbd0-4f5b-b38e-938c64657e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.2\n",
    "r = .4\n",
    "D = 10000  # Dimensionality of data\n",
    "sigma_eps = 0\n",
    "\n",
    "sigma_s, w_star = LearningCurveExperiments.makeGaussianParams(D, alpha, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0289c251-b983-4325-a360-f62f41d1afdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, y_train = DatasetMaker.makeGaussianDataset_lin(max(P_list)*num_trials, w_star, sigma_s, sigma_eps = sigma_eps)\n",
    "X_test, y_test = DatasetMaker.makeGaussianDataset_lin(PTest, w_star, sigma_s, sigma_eps = sigma_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9485f57d-ed0a-4c60-9003-cb1634b09222",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 1/10\n",
      "Completed trial 1/10\n",
      "Starting trial 2/10\n",
      "Completed trial 2/10\n",
      "Starting trial 3/10\n",
      "Completed trial 3/10\n",
      "Starting trial 4/10\n",
      "Completed trial 4/10\n",
      "Starting trial 5/10\n",
      "Completed trial 5/10\n",
      "Starting trial 6/10\n",
      "Completed trial 6/10\n",
      "Starting trial 7/10\n",
      "Completed trial 7/10\n",
      "Starting trial 8/10\n",
      "Completed trial 8/10\n",
      "Starting trial 9/10\n",
      "Completed trial 9/10\n",
      "Starting trial 10/10\n",
      "Completed trial 10/10\n"
     ]
    }
   ],
   "source": [
    "# Train random feature models\n",
    "test_errors = LearningCurveExperiments.train_random_feature_models_fixM(X_train, y_train, X_test, y_test, num_trials, KVals, M, lamVals, P_list, ensErrFuncs, nonlinearity=nonlinearity, Fvar = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "88b5efc1-fd28-4e91-9f6e-bdfa7070b3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate theory curves\n",
    "test_errors_theory, bias_theory, var_theory = LearningCurveExperiments.compute_theoretical_learning_curves(sigma_s.cpu().numpy(), w_star.cpu().numpy(), KVals, lamVals, P_list, M, sigma_eps = sigma_eps, returnBiasVariance = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "643dc032-da86-4a61-a1c2-2243d6990fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment saved to /n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features/RF_Gaussian_alpha1.2_r0.4_M_1024.pkl\n"
     ]
    }
   ],
   "source": [
    "# Define the experiment output and parameters as a dictionary\n",
    "experiment_results = {\n",
    "    'num_trials': num_trials,\n",
    "    'KVals': KVals,\n",
    "    'M': M,\n",
    "    'lamVals': lamVals,\n",
    "    'P_list': P_list,\n",
    "    'PTest': PTest,\n",
    "    'nonlinearity': nonlinearity,\n",
    "    'alpha': alpha,\n",
    "    'r': r,\n",
    "    'D': D,\n",
    "    'test_errors': test_errors,  # Assuming this is the output of train_random_feature_models\n",
    "    'test_errors_theory': test_errors_theory\n",
    "}\n",
    "\n",
    "# Create a descriptive filename with a timestamp to avoid overwriting\n",
    "#timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#filename = f'RF_Gaussian_{timestamp}.pkl'\n",
    "filename = rf'RF_Gaussian_alpha{alpha}_r{r}_M_{M}.pkl'\n",
    "\n",
    "# Define the save path\n",
    "save_path = '/n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open(os.path.join(save_path, filename), 'wb') as f:\n",
    "    pickle.dump(experiment_results, f)\n",
    "\n",
    "print(f\"Experiment saved to {os.path.join(save_path, filename)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b9c117-70b8-46bc-818e-38063fee8798",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CIFAR 10 Ensemble Regression: Sweeep K with P and M fixed (multiple P values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "472af134-fdf8-45fe-a330-1504932a51f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_trials = 50\n",
    "KVals = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "M = 2**10\n",
    "lamVals = np.logspace(-4, 2, 50)\n",
    "P_list = [2**7, 2**10, 2**13]\n",
    "PTest = 10000\n",
    "nonlinearity = torch.relu  # Relu random features.\n",
    "ensErrFuncs = [(auxFuncs.mean, auxFuncs.SquareError), (auxFuncs.mean, auxFuncs.SgnErrorRate), (auxFuncs.majorityVote, auxFuncs.SgnErrorRate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "201657c3-ca26-4091-ad0b-e10ad5fb5791",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/n/holystore01/LABS/pehlevan_lab/Everyone/cifar' #Location of the cifar 10 dataset.\n",
    "class_groups =  [[0,1,7,8,9], [2,3,4,5,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c61304f-64ad-45cb-ae6a-95a620f1ed9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "X_train_np, y_train_np, X_test_np, y_test_np = DatasetMaker.get_binarized_CIFAR10(data_root, class_groups, flatten=True, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8591852d-122b-4d9e-a772-9ef2fca40ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X_train_np.shape[1]\n",
    "Fvar = 2/D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f27a566-9263-4f6f-9e55-50d92e5b4a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to torch tensors\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float64).cuda()\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.float64).cuda().reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test_np, dtype=torch.float64).cuda()\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.float64).cuda().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19764e02-dd22-4a78-919f-73b1c32896a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 1/50\n",
      "Completed trial 1/50\n",
      "Starting trial 2/50\n",
      "Completed trial 2/50\n",
      "Starting trial 3/50\n",
      "Completed trial 3/50\n",
      "Starting trial 4/50\n",
      "Completed trial 4/50\n",
      "Starting trial 5/50\n",
      "Completed trial 5/50\n",
      "Starting trial 6/50\n",
      "Completed trial 6/50\n",
      "Starting trial 7/50\n",
      "Completed trial 7/50\n",
      "Starting trial 8/50\n",
      "Completed trial 8/50\n",
      "Starting trial 9/50\n",
      "Completed trial 9/50\n",
      "Starting trial 10/50\n",
      "Completed trial 10/50\n",
      "Starting trial 11/50\n",
      "Completed trial 11/50\n",
      "Starting trial 12/50\n",
      "Completed trial 12/50\n",
      "Starting trial 13/50\n",
      "Completed trial 13/50\n",
      "Starting trial 14/50\n",
      "Completed trial 14/50\n",
      "Starting trial 15/50\n",
      "Completed trial 15/50\n",
      "Starting trial 16/50\n",
      "Completed trial 16/50\n",
      "Starting trial 17/50\n",
      "Completed trial 17/50\n",
      "Starting trial 18/50\n",
      "Completed trial 18/50\n",
      "Starting trial 19/50\n",
      "Completed trial 19/50\n",
      "Starting trial 20/50\n",
      "Completed trial 20/50\n",
      "Starting trial 21/50\n",
      "Completed trial 21/50\n",
      "Starting trial 22/50\n",
      "Completed trial 22/50\n",
      "Starting trial 23/50\n",
      "Completed trial 23/50\n",
      "Starting trial 24/50\n",
      "Completed trial 24/50\n",
      "Starting trial 25/50\n",
      "Completed trial 25/50\n",
      "Starting trial 26/50\n",
      "Completed trial 26/50\n",
      "Starting trial 27/50\n",
      "Completed trial 27/50\n",
      "Starting trial 28/50\n",
      "Completed trial 28/50\n",
      "Starting trial 29/50\n",
      "Completed trial 29/50\n",
      "Starting trial 30/50\n",
      "Completed trial 30/50\n",
      "Starting trial 31/50\n",
      "Completed trial 31/50\n",
      "Starting trial 32/50\n",
      "Completed trial 32/50\n",
      "Starting trial 33/50\n",
      "Completed trial 33/50\n",
      "Starting trial 34/50\n",
      "Completed trial 34/50\n",
      "Starting trial 35/50\n",
      "Completed trial 35/50\n",
      "Starting trial 36/50\n",
      "Completed trial 36/50\n",
      "Starting trial 37/50\n",
      "Completed trial 37/50\n",
      "Starting trial 38/50\n",
      "Completed trial 38/50\n",
      "Starting trial 39/50\n",
      "Completed trial 39/50\n",
      "Starting trial 40/50\n",
      "Completed trial 40/50\n",
      "Starting trial 41/50\n",
      "Completed trial 41/50\n",
      "Starting trial 42/50\n",
      "Completed trial 42/50\n",
      "Starting trial 43/50\n",
      "Completed trial 43/50\n",
      "Starting trial 44/50\n",
      "Completed trial 44/50\n",
      "Starting trial 45/50\n",
      "Completed trial 45/50\n",
      "Starting trial 46/50\n",
      "Completed trial 46/50\n",
      "Starting trial 47/50\n",
      "Completed trial 47/50\n",
      "Starting trial 48/50\n",
      "Completed trial 48/50\n",
      "Starting trial 49/50\n",
      "Completed trial 49/50\n",
      "Starting trial 50/50\n",
      "Completed trial 50/50\n"
     ]
    }
   ],
   "source": [
    "test_errors = LearningCurveExperiments.train_random_feature_models_fixM(X_train, y_train, X_test, y_test, num_trials, KVals, M, lamVals, P_list, ensErrFuncs, nonlinearity=nonlinearity, Fvar = Fvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04e2fc3f-9b8b-4a28-9c55-12a8cc122f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical learning curves computed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the subset size and file directory\n",
    "subset_size = 30000  # Set your specific subset size\n",
    "save_dir = \"KernelSpectra\"\n",
    "\n",
    "# Load the saved dictionary for the given subset size\n",
    "with open(os.path.join(save_dir, f'CIFAR_kernel_results_{subset_size}.pkl'), 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# Retrieve eigenvalues, wbar, and sigma_eps from the dictionary\n",
    "eigvals = results['eigvals'].reshape(-1)\n",
    "wbar = results['wbar'].reshape(-1)\n",
    "sigma_eps = results['sigma_eps']\n",
    "\n",
    "# Compute theoretical learning curves using the retrieved eigenvalues, wbar, and sigma_eps\n",
    "test_errors_theory, bias_theory, var_theory = LearningCurveExperiments.compute_theoretical_learning_curves_fixM(\n",
    "    Sigma=eigvals,       # Sigma corresponds to the kernel eigenvalues\n",
    "    wbar=wbar,           # Ground truth weights from CIFAR10 task\n",
    "    KVals=KVals,         # Number of ensemble models\n",
    "    lamVals=lamVals,     # Ridge regularization values\n",
    "    P_list=P_list,       # Different training sample sizes\n",
    "    M=M,                 # Total number of random features\n",
    "    sigma_eps = sigma_eps,        #noise level in the task.\n",
    "    returnBiasVariance = True\n",
    ")\n",
    "\n",
    "# Print or save the theoretical learning curves as needed\n",
    "print(\"Theoretical learning curves computed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04f72dff-f59c-4b95-8289-86d9a33aaca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment saved to /n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features/RF_CIFAR.pkl\n"
     ]
    }
   ],
   "source": [
    "# Define the experiment output and parameters as a dictionary\n",
    "experiment_results = {\n",
    "    'num_trials': num_trials,\n",
    "    'KVals': KVals,\n",
    "    'M': M,\n",
    "    'lamVals': lamVals,\n",
    "    'P_list': P_list,\n",
    "    'PTest': PTest,\n",
    "    'nonlinearity': nonlinearity,\n",
    "    'class_groups': class_groups,  # Class groups for CIFAR-10 binarization\n",
    "    'test_errors': test_errors,  # Numerical results from random feature models\n",
    "    'test_errors_theory': test_errors_theory,  # Theoretical results\n",
    "    'bias_theory': bias_theory,\n",
    "    'var_theory': var_theory,\n",
    "    'ensErrFuncs': ensErrFuncs\n",
    "}\n",
    "\n",
    "# Create a descriptive filename with a timestamp to avoid overwriting\n",
    "#timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#filename = f'RF_CIFAR_{timestamp}.pkl'\n",
    "filename = f'RF_CIFAR.pkl'\n",
    "\n",
    "# Define the save path\n",
    "save_path = '/n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open(os.path.join(save_path, filename), 'wb') as f:\n",
    "    pickle.dump(experiment_results, f)\n",
    "\n",
    "print(f\"Experiment saved to {os.path.join(save_path, filename)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3609d4-ffb0-4008-983c-9c7fef2bf730",
   "metadata": {},
   "source": [
    "## MNIST Ensemble Experiments sweeping K with M fixed (multiple P values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a242483-f0ae-4ab7-8874-aa932658b754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_trials = 50\n",
    "KVals = [1, 2, 4, 8, 16, 32, 64, 128]\n",
    "M = 2**10\n",
    "lamVals = np.logspace(-4, 2, 50)\n",
    "P_list = [2**7, 2**10, 2**13]\n",
    "PTest = 10000\n",
    "nonlinearity = torch.relu  # Relu random features.\n",
    "ensErrFuncs = [(auxFuncs.mean, auxFuncs.SquareError), (auxFuncs.mean, auxFuncs.SgnErrorRate), (auxFuncs.majorityVote, auxFuncs.SgnErrorRate)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf18c423-97a6-4fa7-a31f-112bebb2107a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/n/holystore01/LABS/pehlevan_lab/Lab/bruben/Datasets/MNIST'  # Location to store MNIST dataset\n",
    "class_groups = [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]  # Group digits 0-4 and 5-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c29f732-98d5-4fe9-97cd-29b1d957185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load binarized MNIST data\n",
    "X_train_np, y_train_np, X_test_np, y_test_np = DatasetMaker.get_binarized_MNIST(\n",
    "    data_root, class_groups, flatten=True, normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc33bc19-a69b-499f-a41f-5cf3efdc051b",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X_train_np.shape[1]\n",
    "Fvar = 2/D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55c24675-54dd-492d-a313-e0018b0cef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to torch tensors\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float64).cuda()\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.float64).cuda().reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test_np, dtype=torch.float64).cuda()\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.float64).cuda().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdcff145-e71e-4259-93ed-93f94b3a4a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 1/50\n",
      "Completed trial 1/50\n",
      "Starting trial 2/50\n",
      "Completed trial 2/50\n",
      "Starting trial 3/50\n",
      "Completed trial 3/50\n",
      "Starting trial 4/50\n",
      "Completed trial 4/50\n",
      "Starting trial 5/50\n",
      "Completed trial 5/50\n",
      "Starting trial 6/50\n",
      "Completed trial 6/50\n",
      "Starting trial 7/50\n",
      "Completed trial 7/50\n",
      "Starting trial 8/50\n",
      "Completed trial 8/50\n",
      "Starting trial 9/50\n",
      "Completed trial 9/50\n",
      "Starting trial 10/50\n",
      "Completed trial 10/50\n",
      "Starting trial 11/50\n",
      "Completed trial 11/50\n",
      "Starting trial 12/50\n",
      "Completed trial 12/50\n",
      "Starting trial 13/50\n",
      "Completed trial 13/50\n",
      "Starting trial 14/50\n",
      "Completed trial 14/50\n",
      "Starting trial 15/50\n",
      "Completed trial 15/50\n",
      "Starting trial 16/50\n",
      "Completed trial 16/50\n",
      "Starting trial 17/50\n",
      "Completed trial 17/50\n",
      "Starting trial 18/50\n",
      "Completed trial 18/50\n",
      "Starting trial 19/50\n",
      "Completed trial 19/50\n",
      "Starting trial 20/50\n",
      "Completed trial 20/50\n",
      "Starting trial 21/50\n",
      "Completed trial 21/50\n",
      "Starting trial 22/50\n",
      "Completed trial 22/50\n",
      "Starting trial 23/50\n",
      "Completed trial 23/50\n",
      "Starting trial 24/50\n",
      "Completed trial 24/50\n",
      "Starting trial 25/50\n",
      "Completed trial 25/50\n",
      "Starting trial 26/50\n",
      "Completed trial 26/50\n",
      "Starting trial 27/50\n",
      "Completed trial 27/50\n",
      "Starting trial 28/50\n",
      "Completed trial 28/50\n",
      "Starting trial 29/50\n",
      "Completed trial 29/50\n",
      "Starting trial 30/50\n",
      "Completed trial 30/50\n",
      "Starting trial 31/50\n",
      "Completed trial 31/50\n",
      "Starting trial 32/50\n",
      "Completed trial 32/50\n",
      "Starting trial 33/50\n",
      "Completed trial 33/50\n",
      "Starting trial 34/50\n",
      "Completed trial 34/50\n",
      "Starting trial 35/50\n",
      "Completed trial 35/50\n",
      "Starting trial 36/50\n",
      "Completed trial 36/50\n",
      "Starting trial 37/50\n",
      "Completed trial 37/50\n",
      "Starting trial 38/50\n",
      "Completed trial 38/50\n",
      "Starting trial 39/50\n",
      "Completed trial 39/50\n",
      "Starting trial 40/50\n",
      "Completed trial 40/50\n",
      "Starting trial 41/50\n",
      "Completed trial 41/50\n",
      "Starting trial 42/50\n",
      "Completed trial 42/50\n",
      "Starting trial 43/50\n",
      "Completed trial 43/50\n",
      "Starting trial 44/50\n",
      "Completed trial 44/50\n",
      "Starting trial 45/50\n",
      "Completed trial 45/50\n",
      "Starting trial 46/50\n",
      "Completed trial 46/50\n",
      "Starting trial 47/50\n",
      "Completed trial 47/50\n",
      "Starting trial 48/50\n",
      "Completed trial 48/50\n",
      "Starting trial 49/50\n",
      "Completed trial 49/50\n",
      "Starting trial 50/50\n",
      "Completed trial 50/50\n"
     ]
    }
   ],
   "source": [
    "test_errors = LearningCurveExperiments.train_random_feature_models_fixM(X_train, y_train, X_test, y_test, num_trials, KVals, M, lamVals, P_list, ensErrFuncs, nonlinearity=nonlinearity, Fvar = Fvar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "23ff2e0f-d52b-4ddd-b385-89b01e65c7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical learning curves computed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the subset size and file directory\n",
    "subset_size = 30000  # Set your specific subset size\n",
    "save_dir = \"KernelSpectra\"\n",
    "\n",
    "# Load the saved dictionary for the given subset size\n",
    "with open(os.path.join(save_dir, f'MNIST_kernel_results_{subset_size}.pkl'), 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# Retrieve eigenvalues, wbar, and sigma_eps from the dictionary\n",
    "eigvals = results['eigvals'].reshape(-1)\n",
    "wbar = results['wbar'].reshape(-1)\n",
    "sigma_eps = results['sigma_eps']\n",
    "\n",
    "# Compute theoretical learning curves using the retrieved eigenvalues, wbar, and sigma_eps\n",
    "test_errors_theory, bias_theory, var_thoery = LearningCurveExperiments.compute_theoretical_learning_curves_fixM(\n",
    "    Sigma=eigvals,       # Sigma corresponds to the kernel eigenvalues\n",
    "    wbar=wbar,           # Ground truth weights from CIFAR10 task\n",
    "    KVals=KVals,         # Number of ensemble models\n",
    "    lamVals=lamVals,     # Ridge regularization values\n",
    "    P_list=P_list,       # Different training sample sizes\n",
    "    M=M,                 # Total number of random features\n",
    "    sigma_eps = sigma_eps,        #noise level in the task.\n",
    "    returnBiasVariance = True\n",
    ")\n",
    "\n",
    "# Print or save the theoretical learning curves as needed\n",
    "print(\"Theoretical learning curves computed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17a022d6-e46a-471b-bb93-38767ead223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment saved to /n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features/RF_MNIST.pkl\n"
     ]
    }
   ],
   "source": [
    "# Define the experiment output and parameters as a dictionary\n",
    "experiment_results = {\n",
    "    'num_trials': num_trials,\n",
    "    'KVals': KVals,\n",
    "    'M': M,\n",
    "    'lamVals': lamVals,\n",
    "    'P_list': P_list,\n",
    "    'PTest': PTest,\n",
    "    'nonlinearity': nonlinearity,\n",
    "    'class_groups': class_groups,  # Class groups for CIFAR-10 binarization\n",
    "    'test_errors': test_errors,  # Numerical results from random feature models\n",
    "    'test_errors_theory': test_errors_theory,  # Theoretical results\n",
    "    'bias_theory': bias_theory,\n",
    "    'var_theory': var_theory,\n",
    "    'ensErrFuncs': ensErrFuncs\n",
    "}\n",
    "\n",
    "# Create a descriptive filename with a timestamp to avoid overwriting\n",
    "#timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "#filename = f'RF_CIFAR_{timestamp}.pkl'\n",
    "filename = f'RF_MNIST.pkl'\n",
    "\n",
    "# Define the save path\n",
    "save_path = '/n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open(os.path.join(save_path, filename), 'wb') as f:\n",
    "    pickle.dump(experiment_results, f)\n",
    "\n",
    "print(f\"Experiment saved to {os.path.join(save_path, filename)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db394126-1efe-4278-baa4-3a961a6bf1e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Figure 3: Scaling Laws for Random Feature EnsemblesensErrFuncs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f618e915-4f2b-47ac-9c80-e14623e6970d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Gaussian Data Experiment Sweeping $\\ell$ and $M$.  Adding a sweep over $r$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6cf7d80c-b8fb-42ed-b198-088214eedbb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'EnsRFTheory' from '/n/home07/bruben/Simulations/Ensemble_DeepLearning/RandomFeatures/EnsRFTheory.py'>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(LearningCurveExperiments)\n",
    "importlib.reload(EnsembleRFs)\n",
    "importlib.reload(EnsRFTheory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8c735b69-357d-427a-aa49-3273ce306017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_trials = 5\n",
    "M_list = np.unique(np.round(np.logspace(2, 3, 6)))\n",
    "ell_list = np.linspace(.2, 1, 9)\n",
    "lamVals = np.logspace(-3, 3, 100)\n",
    "P = 30000\n",
    "nonlinearity = None  # Assuming linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f9a17b0d-ab20-4a70-a977-7f0dec92a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1.5\n",
    "r_list = [.4, .8, 1.2]\n",
    "D = 30000  # Dimensionality of data\n",
    "sigma_eps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b4fff747-1d8b-4550-bb70-a3936e3d47e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting r = 0.4\n",
      "Starting trial 1/5\n",
      "Starting M = 100.0\n",
      "Starting M = 158.0\n",
      "Starting M = 251.0\n",
      "Starting M = 398.0\n",
      "Starting M = 631.0\n",
      "Starting M = 1000.0\n",
      "Completed trial 1/5\n",
      "Starting trial 2/5\n",
      "Starting M = 100.0\n",
      "Starting M = 158.0\n",
      "Starting M = 251.0\n",
      "Starting M = 398.0\n",
      "Starting M = 631.0\n",
      "Starting M = 1000.0\n",
      "Completed trial 2/5\n",
      "Starting trial 3/5\n",
      "Starting M = 100.0\n",
      "Starting M = 158.0\n",
      "Starting M = 251.0\n",
      "Starting M = 398.0\n",
      "Starting M = 631.0\n",
      "Starting M = 1000.0\n",
      "Completed trial 3/5\n",
      "Starting trial 4/5\n",
      "Starting M = 100.0\n",
      "Starting M = 158.0\n",
      "Starting M = 251.0\n",
      "Starting M = 398.0\n",
      "Starting M = 631.0\n",
      "Starting M = 1000.0\n",
      "Completed trial 4/5\n",
      "Starting trial 5/5\n",
      "Starting M = 100.0\n",
      "Starting M = 158.0\n",
      "Starting M = 251.0\n",
      "Starting M = 398.0\n",
      "Starting M = 631.0\n",
      "Starting M = 1000.0\n",
      "Completed trial 5/5\n",
      "Experiment saved to /n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features/RF_Gaussian_alpha1.5_r0.4_M_100-1000_P_30000_trials_5.pkl\n",
      "Starting r = 0.8\n",
      "Starting trial 1/5\n",
      "Starting M = 100.0\n",
      "Starting M = 158.0\n",
      "Starting M = 251.0\n",
      "Starting M = 398.0\n",
      "Starting M = 631.0\n",
      "Starting M = 1000.0\n",
      "Completed trial 1/5\n",
      "Starting trial 2/5\n",
      "Starting M = 100.0\n",
      "Starting M = 158.0\n",
      "Starting M = 251.0\n",
      "Starting M = 398.0\n",
      "Starting M = 631.0\n",
      "Starting M = 1000.0\n",
      "Completed trial 2/5\n",
      "Starting trial 3/5\n",
      "Starting M = 100.0\n",
      "Starting M = 158.0\n",
      "Starting M = 251.0\n",
      "Starting M = 398.0\n",
      "Starting M = 631.0\n",
      "Starting M = 1000.0\n",
      "Completed trial 3/5\n",
      "Starting trial 4/5\n",
      "Starting M = 100.0\n",
      "Starting M = 158.0\n",
      "Starting M = 251.0\n",
      "Starting M = 398.0\n",
      "Starting M = 631.0\n",
      "Starting M = 1000.0\n",
      "Completed trial 4/5\n",
      "Starting trial 5/5\n",
      "Starting M = 100.0\n",
      "Starting M = 158.0\n",
      "Starting M = 251.0\n",
      "Starting M = 398.0\n",
      "Starting M = 631.0\n",
      "Starting M = 1000.0\n",
      "Completed trial 5/5\n",
      "Experiment saved to /n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features/RF_Gaussian_alpha1.5_r0.8_M_100-1000_P_30000_trials_5.pkl\n",
      "Starting r = 1.2\n",
      "Starting trial 1/5\n",
      "Starting M = 100.0\n",
      "Starting M = 158.0\n",
      "Starting M = 251.0\n",
      "Starting M = 398.0\n",
      "Starting M = 631.0\n",
      "Starting M = 1000.0\n",
      "Completed trial 1/5\n",
      "Starting trial 2/5\n",
      "Starting M = 100.0\n",
      "Starting M = 158.0\n",
      "Starting M = 251.0\n",
      "Starting M = 398.0\n",
      "Starting M = 631.0\n",
      "Starting M = 1000.0\n",
      "Completed trial 2/5\n",
      "Starting trial 3/5\n",
      "Starting M = 100.0\n",
      "Starting M = 158.0\n",
      "Starting M = 1000.0\n",
      "Completed trial 5/5\n",
      "Experiment saved to /n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features/RF_Gaussian_alpha1.5_r1.2_M_100-1000_P_30000_trials_5.pkl\n"
     ]
    }
   ],
   "source": [
    "for r in r_list:\n",
    "    print('Starting r = ' + str(r))\n",
    "\n",
    "    # Perform training using the new function\n",
    "    test_errors = LearningCurveExperiments.train_random_feature_models_ell_synthetic(\n",
    "        num_trials,\n",
    "        ell_list,\n",
    "        M_list,\n",
    "        P,\n",
    "        lamVals,\n",
    "        D,\n",
    "        alpha,\n",
    "        r,\n",
    "        Fvar=1,\n",
    "        sigma_eps=sigma_eps\n",
    "    )\n",
    "\n",
    "    # Generate sigma_s and w_star for theoretical computations\n",
    "    sigma_s, w_star = LearningCurveExperiments.makeGaussianParams(D, alpha, r)\n",
    "\n",
    "    # Perform theoretical computations\n",
    "    #NOTE: During Theory Computations, you must use fractional values for N and K to get the true exponents.  This is ok because only the ratios matter.\n",
    "    test_errors_theory = LearningCurveExperiments.compute_theoretical_learning_curves_ell(\n",
    "        sigma_s.cpu().numpy(),\n",
    "        w_star.cpu().numpy(),\n",
    "        ell_list,\n",
    "        lamVals,\n",
    "        P,\n",
    "        M_list,\n",
    "        sigma_eps=sigma_eps\n",
    "    )\n",
    "\n",
    "    # Define the experiment output and parameters as a dictionary\n",
    "    experiment_results = {\n",
    "        'num_trials': num_trials,\n",
    "        'ell_list': ell_list,\n",
    "        'M_list': M_list,\n",
    "        'lamVals': lamVals,\n",
    "        'P': P,\n",
    "        'nonlinearity': nonlinearity,\n",
    "        'alpha': alpha,\n",
    "        'r': r,\n",
    "        'D': D,\n",
    "        'sigma_eps': sigma_eps,\n",
    "        'test_errors': test_errors,\n",
    "        'test_errors_theory': test_errors_theory\n",
    "    }\n",
    "\n",
    "    # Include key parameters in the filename for clarity\n",
    "    M_min = int(M_list[0])\n",
    "    M_max = int(M_list[-1])\n",
    "    filename = f'RF_Gaussian_alpha{alpha}_r{r}_M_{M_min}-{M_max}_P_{P}_trials_{num_trials}.pkl'\n",
    "\n",
    "    # Define the save path\n",
    "    save_path = '/n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features'\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # Save the dictionary as a pickle file\n",
    "    with open(os.path.join(save_path, filename), 'wb') as f:\n",
    "        pickle.dump(experiment_results, f)\n",
    "\n",
    "    print(f\"Experiment saved to {os.path.join(save_path, filename)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c03e0-cf11-4393-a298-ed6e89673bbf",
   "metadata": {},
   "source": [
    "# CIFAR 10 Experiment Sweeping $M$, $\\ell$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "02e9b8c8-e89d-40c2-be4d-26b6015c750a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "num_trials = 10\n",
    "M_list = np.unique(np.round(np.logspace(2, 3, 15))).astype(int)  # M values from 1e3 to 1e4\n",
    "ell_list = np.linspace(.1, 1, 10)  # 11 values between 0 and 1\n",
    "lamVals = np.logspace(-4, 2, 50)\n",
    "P = 50000  # Fixed training sample size -- use the whole training set\n",
    "PTest = 10000  #Use the whole test set.\n",
    "nonlinearity = torch.relu  # ReLU random features\n",
    "\n",
    "# Data root and class groups for CIFAR-10 binarization\n",
    "data_root = '/n/holystore01/LABS/pehlevan_lab/Everyone/cifar'  # Location of the CIFAR-10 dataset\n",
    "class_groups = [[0, 1, 7, 8, 9], [2, 3, 4, 5, 6]]\n",
    "\n",
    "# Load and preprocess CIFAR-10 data\n",
    "X_train_np, y_train_np, X_test_np, y_test_np = DatasetMaker.get_binarized_CIFAR10(\n",
    "    data_root, class_groups, flatten=True, normalize=True\n",
    ")\n",
    "\n",
    "D = X_train_np.shape[1]\n",
    "Fvar = 2 / D  # Variance for random features\n",
    "\n",
    "# Convert data to torch tensors and move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float64).to(device)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.float64).to(device).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test_np, dtype=torch.float64).to(device)\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.float64).to(device).reshape(-1, 1)\n",
    "\n",
    "# Ensure that the training data is large enough for all trials\n",
    "# total_train_samples = P * num_trials\n",
    "# if X_train.shape[0] < total_train_samples:\n",
    "#     # Optionally, you can augment the training data here\n",
    "#     raise ValueError(\"Not enough training samples for the specified number of trials.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f35ab3e-486a-4c60-b0a8-6de3627aa772",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 1/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 1/10\n",
      "Starting trial 2/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 2/10\n",
      "Starting trial 3/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 3/10\n",
      "Starting trial 4/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 4/10\n",
      "Starting trial 5/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 5/10\n",
      "Starting trial 6/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 6/10\n",
      "Starting trial 7/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 7/10\n",
      "Starting trial 8/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 8/10\n",
      "Starting trial 9/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 9/10\n",
      "Starting trial 10/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 10/10\n"
     ]
    }
   ],
   "source": [
    "# Run experiments using the new ell sweep function\n",
    "test_errors = LearningCurveExperiments.train_random_feature_models_ell(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    num_trials, ell_list, M_list, P, lamVals,\n",
    "    nonlinearity=nonlinearity, Fvar=Fvar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7ed7d0b-f9e2-48b9-9ff4-16ffe39a761c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'LearningCurveExperiments' from '/n/home07/bruben/Simulations/Ensemble_DeepLearning/RandomFeatures/LearningCurveExperiments.py'>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(EnsRFTheory)\n",
    "importlib.reload(LearningCurveExperiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "58fa6d60-74a8-46ac-83f5-00ab19304e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical learning curves computed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the subset size and file directory\n",
    "subset_size = 30000  # Set your specific subset size\n",
    "save_dir = \"KernelSpectra\"\n",
    "\n",
    "# Load the saved dictionary for the given subset size\n",
    "with open(os.path.join(save_dir, f'CIFAR_kernel_results_{subset_size}.pkl'), 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# Retrieve eigenvalues, wbar, and sigma_eps from the dictionary\n",
    "eigvals = results['eigvals'].reshape(-1)\n",
    "wbar = results['wbar'].reshape(-1)\n",
    "sigma_eps = results['sigma_eps']\n",
    "\n",
    "# Compute theoretical learning curves using the retrieved eigenvalues, wbar, and sigma_eps\n",
    "test_errors_theory = LearningCurveExperiments.compute_theoretical_learning_curves_ell(\n",
    "    Sigma=eigvals,       # Sigma corresponds to the kernel eigenvalues\n",
    "    wbar=wbar,           # Ground truth weights from CIFAR-10 task\n",
    "    ell_list=ell_list,   # List of ell values\n",
    "    lamVals=lamVals,     # Ridge regularization values\n",
    "    P=P,                 # Fixed training sample size\n",
    "    M_list=M_list,       # List of total number of random features\n",
    "    sigma_eps=sigma_eps  # Noise level in the task\n",
    ")\n",
    "\n",
    "# Print or save the theoretical learning curves as needed\n",
    "print(\"Theoretical learning curves computed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c3627b53-78b0-40d8-80db-16af4570c447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment saved to /n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features/RF_CIFAR_M_100-1000_P_50000_trials_10.pkl\n"
     ]
    }
   ],
   "source": [
    "# Define the experiment output and parameters as a dictionary\n",
    "experiment_results = {\n",
    "    'num_trials': num_trials,\n",
    "    'ell_list': ell_list,\n",
    "    'M_list': M_list,\n",
    "    'lamVals': lamVals,\n",
    "    'P': P,\n",
    "    'PTest': PTest,\n",
    "    'nonlinearity': nonlinearity,\n",
    "    'class_groups': class_groups,  # Class groups for CIFAR-10 binarization\n",
    "    'test_errors': test_errors,          # Numerical results from random feature models\n",
    "    'test_errors_theory': test_errors_theory  # Theoretical results\n",
    "}\n",
    "\n",
    "# Create a descriptive filename with a timestamp to avoid overwriting\n",
    "M_min = int(M_list[0])\n",
    "M_max = int(M_list[-1])\n",
    "filename = f'RF_CIFAR_M_{M_min}-{M_max}_P_{P}_trials_{num_trials}.pkl'\n",
    "\n",
    "# Define the save path\n",
    "save_path = '/n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open(os.path.join(save_path, filename), 'wb') as f:\n",
    "    pickle.dump(experiment_results, f)\n",
    "\n",
    "print(f\"Experiment saved to {os.path.join(save_path, filename)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc9cd93-198c-430c-8602-17c03f72b42e",
   "metadata": {},
   "source": [
    "# MNIST Experiment Sweeping $M$, $\\ell$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f05c79b-7539-410f-893b-1145fb0e90b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = '/n/holystore01/LABS/pehlevan_lab/Lab/bruben/Datasets/MNIST'  # Location to store MNIST dataset\n",
    "class_groups = [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]  # Group digits 0-4 and 5-9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65a71353-d3c6-43bb-b8f9-ad6cb6374f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load binarized MNIST data\n",
    "X_train_np, y_train_np, X_test_np, y_test_np = DatasetMaker.get_binarized_MNIST(\n",
    "    data_root, class_groups, flatten=True, normalize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16291b10-41ca-40de-99a5-e45d952a9dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "D = X_train_np.shape[1]\n",
    "Fvar = 2/D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "93f2371a-acef-4c3a-94a3-1c97b16869b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to torch tensors\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float64).cuda()\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.float64).cuda().reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test_np, dtype=torch.float64).cuda()\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.float64).cuda().reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "824052de-deb3-4575-adb1-7ddeee6d1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 10\n",
    "M_list = np.unique(np.round(np.logspace(2, 3, 15))).astype(int)  # M values from 1e3 to 1e4\n",
    "ell_list = np.linspace(.1, 1, 10)  # 11 values between 0 and 1\n",
    "lamVals = np.logspace(-4, 2, 50)\n",
    "P = 50000  # Fixed training sample size -- use the whole training set\n",
    "PTest = 10000  #Use the whole test set.\n",
    "nonlinearity = torch.relu  # ReLU random features\n",
    "\n",
    "# Data root and class groups for CIFAR-10 binarization\n",
    "data_root = '/n/holystore01/LABS/pehlevan_lab/Lab/bruben/Datasets/MNIST'  # Location to store MNIST dataset\n",
    "class_groups = [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]  # Group digits 0-4 and 5-9\n",
    "\n",
    "# Load and preprocess CIFAR-10 data\n",
    "X_train_np, y_train_np, X_test_np, y_test_np = DatasetMaker.get_binarized_MNIST(\n",
    "    data_root, class_groups, flatten=True, normalize=True\n",
    ")\n",
    "\n",
    "D = X_train_np.shape[1]\n",
    "Fvar = 2 / D  # Variance for random features\n",
    "\n",
    "# Convert data to torch tensors and move to GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "X_train = torch.tensor(X_train_np, dtype=torch.float64).to(device)\n",
    "y_train = torch.tensor(y_train_np, dtype=torch.float64).to(device).reshape(-1, 1)\n",
    "X_test = torch.tensor(X_test_np, dtype=torch.float64).to(device)\n",
    "y_test = torch.tensor(y_test_np, dtype=torch.float64).to(device).reshape(-1, 1)\n",
    "\n",
    "# Ensure that the training data is large enough for all trials\n",
    "# total_train_samples = P * num_trials\n",
    "# if X_train.shape[0] < total_train_samples:\n",
    "#     # Optionally, you can augment the training data here\n",
    "#     raise ValueError(\"Not enough training samples for the specified number of trials.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "68701ee0-d944-4c99-b7f6-67cb761d847f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trial 1/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 1/10\n",
      "Starting trial 2/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 2/10\n",
      "Starting trial 3/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 3/10\n",
      "Starting trial 4/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 4/10\n",
      "Starting trial 5/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 5/10\n",
      "Starting trial 6/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 6/10\n",
      "Starting trial 7/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 7/10\n",
      "Starting trial 8/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 8/10\n",
      "Starting trial 9/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 9/10\n",
      "Starting trial 10/10\n",
      "Starting M = 100\n",
      "Starting M = 118\n",
      "Starting M = 139\n",
      "Starting M = 164\n",
      "Starting M = 193\n",
      "Starting M = 228\n",
      "Starting M = 268\n",
      "Starting M = 316\n",
      "Starting M = 373\n",
      "Starting M = 439\n",
      "Starting M = 518\n",
      "Starting M = 611\n",
      "Starting M = 720\n",
      "Starting M = 848\n",
      "Starting M = 1000\n",
      "Completed trial 10/10\n"
     ]
    }
   ],
   "source": [
    "# Run experiments using the new ell sweep function\n",
    "test_errors = LearningCurveExperiments.train_random_feature_models_ell(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    num_trials, ell_list, M_list, P, lamVals,\n",
    "    nonlinearity=nonlinearity, Fvar=Fvar\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "326e2ca0-de2d-425e-be3c-297fcbeb70de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'LearningCurveExperiments' from '/n/home07/bruben/Simulations/Ensemble_DeepLearning/RandomFeatures/LearningCurveExperiments.py'>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(EnsRFTheory)\n",
    "importlib.reload(LearningCurveExperiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "76aeacce-8b08-4ce7-bd9b-fc58adf2adf5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theoretical learning curves computed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define the subset size and file directory\n",
    "subset_size = 30000  # Set your specific subset size\n",
    "save_dir = \"KernelSpectra\"\n",
    "\n",
    "# Load the saved dictionary for the given subset size\n",
    "with open(os.path.join(save_dir, f'MNIST_kernel_results_{subset_size}.pkl'), 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "# Retrieve eigenvalues, wbar, and sigma_eps from the dictionary\n",
    "eigvals = results['eigvals'].reshape(-1)\n",
    "wbar = results['wbar'].reshape(-1)\n",
    "sigma_eps = results['sigma_eps']\n",
    "\n",
    "# Compute theoretical learning curves using the retrieved eigenvalues, wbar, and sigma_eps\n",
    "test_errors_theory = LearningCurveExperiments.compute_theoretical_learning_curves_ell(\n",
    "    Sigma=eigvals,       # Sigma corresponds to the kernel eigenvalues\n",
    "    wbar=wbar,           # Ground truth weights from CIFAR-10 task\n",
    "    ell_list=ell_list,   # List of ell values\n",
    "    lamVals=lamVals,     # Ridge regularization values\n",
    "    P=P,                 # Fixed training sample size\n",
    "    M_list=M_list,       # List of total number of random features\n",
    "    sigma_eps=sigma_eps  # Noise level in the task\n",
    ")\n",
    "\n",
    "# Print or save the theoretical learning curves as needed\n",
    "print(\"Theoretical learning curves computed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56d288f1-9a80-4863-916b-754da4aa464d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment saved to /n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features/RF_MNIST_M_100-1000_P_50000_trials_10.pkl\n"
     ]
    }
   ],
   "source": [
    "# Define the experiment output and parameters as a dictionary\n",
    "experiment_results = {\n",
    "    'num_trials': num_trials,\n",
    "    'ell_list': ell_list,\n",
    "    'M_list': M_list,\n",
    "    'lamVals': lamVals,\n",
    "    'P': P,\n",
    "    'PTest': PTest,\n",
    "    'nonlinearity': nonlinearity,\n",
    "    'class_groups': class_groups,  # Class groups for CIFAR-10 binarization\n",
    "    'test_errors': test_errors,          # Numerical results from random feature models\n",
    "    'test_errors_theory': test_errors_theory  # Theoretical results\n",
    "}\n",
    "\n",
    "# Create a descriptive filename with a timestamp to avoid overwriting\n",
    "M_min = int(M_list[0])\n",
    "M_max = int(M_list[-1])\n",
    "filename = f'RF_MNIST_M_{M_min}-{M_max}_P_{P}_trials_{num_trials}.pkl'\n",
    "\n",
    "# Define the save path\n",
    "save_path = '/n/holystore01/LABS/pehlevan_lab/Lab/bruben/Ensemble_Random_Features'\n",
    "\n",
    "# Ensure the directory exists\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "\n",
    "# Save the dictionary as a pickle file\n",
    "with open(os.path.join(save_path, filename), 'wb') as f:\n",
    "    pickle.dump(experiment_results, f)\n",
    "\n",
    "print(f\"Experiment saved to {os.path.join(save_path, filename)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (TorchReg)",
   "language": "python",
   "name": "torchreg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
